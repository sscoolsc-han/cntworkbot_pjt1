"""
s5_LegalSearchEngine.py
í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰(ë²¡í„° + BM25) + ë²•ë ¹ íŠ¹í™” ê¸°ëŠ¥
"""

import numpy as np
import faiss
from typing import List, Dict, Optional, Callable
from rank_bm25 import BM25Okapi
import re
import json
import pickle
import os

from sentence_transformers import CrossEncoder

class LegalSearchEngine:
    """ë²•ë ¹ íŠ¹í™” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì—”ì§„"""
    
    def __init__(self, 
                 faiss_index: faiss.Index,
                 metadata: List[Dict],  
                 embedding_manager=None,
                 bm25_index_path: Optional[str] = None):
        """
        Args:
            faiss_index: FAISS ì¸ë±ìŠ¤
            metadata: ë©”íƒ€ë°ì´í„° (chunks ì •ë³´ í¬í•¨)
            embedding_manager: EmbeddingManager ì¸ìŠ¤í„´ìŠ¤
            bm25_index_path: BM25 ì¸ë±ìŠ¤ ì €ì¥ ê²½ë¡œ (ì˜ˆ: 'data/vector_store/bm25_index.pkl')
        """
        self.faiss_index = faiss_index
        self.metadata = metadata
        self.embedding_manager = embedding_manager
        self.bm25_index_path = bm25_index_path
        
        # BM25 ì¸ë±ìŠ¤ ë¡œë“œ ë˜ëŠ” ìƒì„±
        print("\nğŸ”§ BM25 ì¸ë±ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
        if bm25_index_path and os.path.exists(bm25_index_path):
            self.load_bm25_index()
        else:
            self.build_bm25_index()
            if bm25_index_path:
                self.save_bm25_index()
        
        print("\nğŸ”§ Reranker ë¡œë”© ì¤‘...")
        self.reranker = CrossEncoder(
            'BAAI/bge-reranker-base',
            max_length=512
        )
        print("  âœ“ Reranker ë¡œë”© ì™„ë£Œ")
        
        print("\nâœ“ LegalSearchEngine ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"  - FAISS ë²¡í„° ìˆ˜: {faiss_index.ntotal}")
        print(f"  - BM25 ë¬¸ì„œ ìˆ˜: {len(self.bm25_corpus)}")
    
    def filter_by_doc_name(self, results: List[Dict], query: str) -> List[Dict]:
        """ì¿¼ë¦¬ì—ì„œ ë¬¸ì„œëª… í‚¤ì›Œë“œ ì¶”ì¶œ â†’ metadata.doc_nameìœ¼ë¡œ í•„í„°ë§"""
        import re
        
        # ì¿¼ë¦¬ í‚¤ì›Œë“œ â†’ doc_name ë§¤ì¹­ íŒ¨í„´ (ìˆœì„œ ì¤‘ìš”: êµ¬ì²´ì ì¸ ê²ƒ ë¨¼ì €)
        doc_mapping = [
            # (ì¿¼ë¦¬ íŒ¨í„´, doc_name íŒ¨í„´)
            (r'ì‹œí–‰ê·œì¹™', r'ì‹œí–‰ê·œì¹™'),
            (r'ì‹œí–‰ë ¹', r'ì‹œí–‰ë ¹'),
            (r'AURI|í•´ì„ë¡€', r'AURI|í•´ì„ë¡€'),
            (r'ê±´ì„¤ì‚°ì—…ê¸°ë³¸ë²•', r'ê±´ì„¤ì‚°ì—…ê¸°ë³¸ë²•'),
            (r'ê±´ì„¤ê¸°ìˆ ', r'ê±´ì„¤ê¸°ìˆ '),
            (r'ì‚°ì—…ì•ˆì „', r'ì‚°ì—…ì•ˆì „'),
            (r'êµ­í† ', r'êµ­í† '),
        ]
        
        # ì¿¼ë¦¬ì—ì„œ ë¬¸ì„œ í‚¤ì›Œë“œ ì°¾ê¸°
        target_pattern = None
        for query_pattern, doc_pattern in doc_mapping:
            if re.search(query_pattern, query, re.IGNORECASE):
                target_pattern = doc_pattern
                break
        
        # í‚¤ì›Œë“œ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ = ê±´ì¶•ë²• ë³¸ë²• (ì‹œí–‰ë ¹/ì‹œí–‰ê·œì¹™ ì œì™¸)
        if not target_pattern:
            target_pattern = r'ê±´ì¶•ë²•'
            exclude_pattern = r'ì‹œí–‰ë ¹|ì‹œí–‰ê·œì¹™'
        else:
            exclude_pattern = None
        
        # í•„í„°ë§
        filtered = []
        for r in results:
            doc_name = r.get('metadata', {}).get('doc_name', '')
            
            # í¬í•¨ ì¡°ê±´
            if not re.search(target_pattern, doc_name, re.IGNORECASE):
                continue
            
            # ì œì™¸ ì¡°ê±´
            if exclude_pattern and re.search(exclude_pattern, doc_name, re.IGNORECASE):
                continue
            
            filtered.append(r)
        
        # í•„í„°ë§ ê²°ê³¼ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
        return filtered if filtered else results

    def tokenize_korean(self, text: str) -> List[str]:
        """í•œê¸€ í…ìŠ¤íŠ¸ í† í°í™”"""
        tokens = re.findall(r'\w+', text.lower())
        return tokens
    
    def build_bm25_index(self):
        """BM25 ì¸ë±ìŠ¤ êµ¬ì¶• (metadataì—ì„œ ì§ì ‘)"""
        print("  â³ BM25 ì¸ë±ìŠ¤ ìƒˆë¡œ ìƒì„± ì¤‘...")
        self.bm25_corpus = []
        
        for item in self.metadata:
            content = item.get('content', '')
            tokens = self.tokenize_korean(content)
            self.bm25_corpus.append(tokens)
        
        self.bm25 = BM25Okapi(self.bm25_corpus)
        print(f"  âœ“ BM25 ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {len(self.bm25_corpus)}ê°œ ë¬¸ì„œ")
    
    def save_bm25_index(self):
        """BM25 ì¸ë±ìŠ¤ë¥¼ pickleë¡œ ì €ì¥"""
        if not self.bm25_index_path:
            print("  âš ï¸  BM25 ì €ì¥ ê²½ë¡œê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(os.path.dirname(self.bm25_index_path), exist_ok=True)
        
        # BM25 ê°ì²´ì™€ corpusë¥¼ í•¨ê»˜ ì €ì¥
        bm25_data = {
            'bm25': self.bm25,
            'corpus': self.bm25_corpus
        }
        
        with open(self.bm25_index_path, 'wb') as f:
            pickle.dump(bm25_data, f)
        
        print(f"  ğŸ’¾ BM25 ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {self.bm25_index_path}")
    
    def load_bm25_index(self):
        """pickleì—ì„œ BM25 ì¸ë±ìŠ¤ ë¡œë”©"""
        print(f"  ğŸ“‚ BM25 ì¸ë±ìŠ¤ ë¡œë”© ì¤‘: {self.bm25_index_path}")
        
        with open(self.bm25_index_path, 'rb') as f:
            bm25_data = pickle.load(f)
        
        self.bm25 = bm25_data['bm25']
        self.bm25_corpus = bm25_data['corpus']
        
        print(f"  âœ“ BM25 ì¸ë±ìŠ¤ ë¡œë”© ì™„ë£Œ: {len(self.bm25_corpus)}ê°œ ë¬¸ì„œ")
    
    def vector_search(self, 
                     query: str,
                     top_k: int = 10) -> List[Dict]:
        """ë²¡í„° ê²€ìƒ‰"""
        if not self.embedding_manager:
            raise ValueError("EmbeddingManagerê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        query_embedding = self.embedding_manager.embed_text(query)
        query_embedding = query_embedding.reshape(1, -1).astype('float32')
        
        search_k = top_k * 10
        distances, indices = self.faiss_index.search(query_embedding, search_k)
        
        results = []
        for idx, distance in zip(indices[0], distances[0]):
            if idx >= len(self.metadata):
                continue
            
            item = self.metadata[idx]
            content = item["content"]
                        
            result = {
                "rank": len(results) + 1,
                "chunk_id": item["chunk_id"],
                "content": content,
                "metadata": item["metadata"],
                "score": float(1 / (1 + distance)),
                "search_type": "vector"
            }
            results.append(result)
            
            if len(results) >= top_k:
                break
        
        return results
    
    def keyword_search(self,
                      query: str,
                      top_k: int = 10) -> List[Dict]:
        """í‚¤ì›Œë“œ ê²€ìƒ‰"""
        query_tokens = self.tokenize_korean(query)
        scores = self.bm25.get_scores(query_tokens)
        
        ranked_indices = np.argsort(scores)[::-1]
        
        results = []
        for idx in ranked_indices:
            if scores[idx] <= 0:
                continue
            
            item = self.metadata[idx]
            content = item["content"]
                        
            result = {
                "rank": len(results) + 1,
                "chunk_id": item["chunk_id"],
                "content": content,
                "metadata": item["metadata"],
                "score": float(scores[idx]),
                "search_type": "keyword"
            }
            results.append(result)
            
            if len(results) >= top_k:
                break
        
        return results
    
    def reciprocal_rank_fusion(self,
                               vector_results: List[Dict],
                               keyword_results: List[Dict],
                               k: int = 60) -> List[Dict]:
        """RRF ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ê²°ê³¼ ìœµí•©"""
        chunk_scores = {}
        chunk_data = {}
        
        for result in vector_results:
            chunk_id = result["chunk_id"]
            rank = result["rank"]
            rrf_score = 1 / (k + rank)
            
            chunk_scores[chunk_id] = chunk_scores.get(chunk_id, 0) + rrf_score
            chunk_data[chunk_id] = result
        
        for result in keyword_results:
            chunk_id = result["chunk_id"]
            rank = result["rank"]
            rrf_score = 1 / (k + rank)
            
            chunk_scores[chunk_id] = chunk_scores.get(chunk_id, 0) + rrf_score
            if chunk_id not in chunk_data:
                chunk_data[chunk_id] = result
        
        sorted_chunks = sorted(chunk_scores.items(), key=lambda x: x[1], reverse=True)
        
        results = []
        for i, (chunk_id, score) in enumerate(sorted_chunks):
            result = chunk_data[chunk_id].copy()
            result["rank"] = i + 1
            result["rrf_score"] = float(score)
            result["search_type"] = "hybrid"
            results.append(result)
        
        return results
    
    def rerank(self, query: str, results: List[Dict], top_k: int = 5) -> List[Dict]:
        """Cross-encoderë¡œ ë¦¬ë­í‚¹"""
        if not results:
            return results        
        pairs = [(query, r['content']) for r in results]
        scores = self.reranker.predict(pairs)        
        for i, result in enumerate(results):
            result['rerank_score'] = float(scores[i])
        reranked = sorted(results, key=lambda x: x['rerank_score'], reverse=True)
        for i, r in enumerate(reranked):
            r['rank'] = i + 1       
        return reranked[:top_k]
    
    def hybrid_search(self,
                    query: str,
                    top_k: int = 10,
                    use_rerank: bool = True,
                    use_bm25: bool = False,
                    progress_callback: Optional[Callable[[str], None]] = None) -> List[Dict]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ + ë¦¬ë­í‚¹
        
        Args:
            query: ê²€ìƒ‰ì–´
            top_k: ìµœì¢… ë°˜í™˜ ê°œìˆ˜
            use_rerank: ë¦¬ë­í‚¹ ì‚¬ìš© ì—¬ë¶€
            progress_callback: ì§„í–‰ ìƒí™© ì½œë°± í•¨ìˆ˜
        """
        
        def update_progress(msg: str):
            if progress_callback:
                progress_callback(msg)

        update_progress("ğŸ” ë²¡í„° ê²€ìƒ‰ ì¤‘...")
        vector_results = self.vector_search(query, top_k=top_k)
        update_progress(f"  âœ“ ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ: {len(vector_results)}ê°œ")
        
        if use_bm25:
            update_progress("ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰ ì¤‘...")
            keyword_results = self.keyword_search(query, top_k=top_k)
            update_progress(f"  âœ“ í‚¤ì›Œë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(keyword_results)}ê°œ")
            
            update_progress("ğŸ”€ ê²€ìƒ‰ ê²°ê³¼ ìœµí•© ì¤‘...")
            hybrid_results = self.reciprocal_rank_fusion(vector_results, keyword_results)
        else:
            hybrid_results = vector_results

        update_progress("ğŸ“‚ ë¬¸ì„œ í•„í„°ë§ ì¤‘...")
        filtered_results = self.filter_by_doc_name(hybrid_results, query)
        update_progress(f"  âœ“ {len(filtered_results)}ê°œ ë¬¸ì„œ ì„ ë³„")
        
        if use_rerank:
            update_progress(f"ğŸ¯ Rerankerë¡œ ì •ë°€ ë¶„ì„ ì¤‘...")
            final_results = self.rerank(query, filtered_results[:10], top_k)
        else:
            final_results = filtered_results[:top_k]
        
        return final_results

def main():
    """í…ŒìŠ¤íŠ¸ ì½”ë“œ"""
    import os
    from s4_EmbeddingManager import EmbeddingManager
    from dotenv import load_dotenv
    
    print("="*80)
    print("ğŸ” ë²•ë ¹ íŠ¹í™” ê²€ìƒ‰ì—”ì§„ í…ŒìŠ¤íŠ¸")
    print("="*80)
    
    load_dotenv()
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    
    if not OPENAI_API_KEY:
        print("\nâœ— ì˜¤ë¥˜: OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(current_dir)
    
    vector_store_dir = os.path.join(project_root, "data", "vector_store", "construction_law")
    cache_dir = os.path.join(project_root, "data", "cache")
    
    index_path = os.path.join(vector_store_dir, "faiss_index.bin")
    metadata_path = os.path.join(vector_store_dir, "metadata.json")
    bm25_index_path = os.path.join(vector_store_dir, "bm25_index.pkl")  # ì¶”ê°€!
    
    print(f"\ní”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}")
    print(f"ë²¡í„° ì €ì¥ì†Œ: {vector_store_dir}")
    print(f"ìºì‹œ ë””ë ‰í† ë¦¬: {cache_dir}")
    
    em = EmbeddingManager(
        openai_api_key=OPENAI_API_KEY,
        institution="construction_law",
        cache_dir=cache_dir 
    )
    
    index = em.load_index(index_path)
    metadata = em.load_metadata(metadata_path)
    
    if index is None or metadata is None:
        print("\nâœ— ì¸ë±ìŠ¤ ë˜ëŠ” ë©”íƒ€ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ë¨¼ì € s4_EmbeddingManager.pyë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return
    
    # SearchEngine ì´ˆê¸°í™” (bm25_index_path ì¶”ê°€!)
    search_engine = LegalSearchEngine(
        faiss_index=index,
        metadata=metadata,
        embedding_manager=em,
        bm25_index_path=bm25_index_path  # ì¶”ê°€!
    )
    
    print("\n" + "="*80)
    print("ğŸ“ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬")
    print("="*80)
    
    query = "ê±´íìœ¨ì€ ì–´ë–»ê²Œ ê³„ì‚°í•˜ë‚˜ìš”?"
    print(f"\nì¿¼ë¦¬: {query}")
    
    results = search_engine.hybrid_search(query, top_k=5)
    
    print(f"\nê²€ìƒ‰ ê²°ê³¼: {len(results)}ê±´\n")
    for result in results:
        print(f"\n[{result['rank']}] {result['chunk_id']}")
        print(f"ë©”íƒ€ë°ì´í„°:")
        print(json.dumps(result['metadata'], indent=2, ensure_ascii=False))
        print(f"\në‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {result['content'][:2000]}...")
        print("-" * 80)

if __name__ == "__main__":
    main()